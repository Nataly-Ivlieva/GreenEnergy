Project Overview

A scalable microservice-based backend platform for processing and reporting renewable energy time-series data.

The system focuses on:

reliability

scalability

maintainability

reproducibility

clear separation of concerns

The platform processes:

simulated renewable energy generation (solar, wind, hydro)

weather-based features

machine learning inference results

The dataset exceeds 1M+ time-stamped records and supports both batch and near real-time processing.

Architecture Overview

The core backend is fully containerized using Docker Compose.

Services communicate via internal Docker networking and are independently deployable.

The architecture separates:

Real-time inference services (containerized)

Offline ML training pipeline (non-containerized batch process)

This mirrors real-world MLOps design patterns.

System Components
1 Generator & Ingestion Service

(Java, Spring Boot â€“ Port 8082)

Simulates renewable energy production

Integrates weather features

Generates historical datasets

Calls ML inference API

Runs on scheduled intervals

2 Backend Service

(Java, Spring Boot â€“ Port 8081)

Stores processed data in PostgreSQL

Performs aggregations

Exposes REST endpoints

Acts as central reporting layer

3 Time-Series Storage

(PostgreSQL â€“ Port 5432)

Stores raw & aggregated data

Persistent via Docker volumes

Accessed via internal hostname: postgres

4 Machine Learning Module (Python)
ðŸ”¹ Online Inference API (Containerized â€“ Port 8000)

Loads serialized .pkl model

Serves predictions via REST

Provides model status endpoint

Internal hostname: ml-api

ðŸ”¹ Offline Training Pipeline (Not Containerized)

Runs locally as a scheduled batch process.

Performs initial historical ingestion

Rebuilds training dataset

Retrains model daily

Saves updated model.pkl artifact

Training uses:

historical file generated by the generator

fresh data from PostgreSQL (daily update)

This separation ensures:

retraining does not interrupt inference

lightweight serving layer

flexible experimentation during model development

5 Frontend (Not Containerized)

Implemented using React / Vue.

Runs via:

npm start

Connects directly to http://localhost:8081

Provides map-based visualization

Includes demo authentication with token storage

Read-only reporting interface

Frontend is intentionally excluded from Docker to simplify development workflow.

Running the Backend Platform

From the project root:

docker compose up --build

Starts:

PostgreSQL

Backend

Generator

ML Inference API

Stop services:

docker compose down

Remove volumes:

docker compose down -v

Local Endpoints
Service URL
Backend API http://localhost:8081

Generator http://localhost:8082

ML API http://localhost:8000
Environment Variables

Configured via Docker Compose:

SPRING_DATASOURCE_URL

SPRING_DATASOURCE_USERNAME

SPRING_DATASOURCE_PASSWORD

INTERSERVICE_ML_BASE_URL

Docker-internal hostnames:

postgres

ml-api

Design Principles

Microservice separation

Containerized backend infrastructure

Internal service discovery via Docker DNS

Offline + online ML workflow separation

Batch vs real-time workload isolation

Reproducible local environment
